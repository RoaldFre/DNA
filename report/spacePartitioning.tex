\subsection{Space Partitioning}

A naive implementation of non-bonded forces would test every possible pair of particles, for a total of $n(n-1)/2$ possibilities, with $n$ the number of interacting particles (for our coarse grained DNA model, $n$ would be the number of sites, which is three times the number of monomers).
However, most of these considered pairs are superfluous because the particles are separated too far apart and their interaction is negligible. This $O\left(\frac{n(n-1)}{2}\right) = O(n^2)$ complexity is undesirable for performance reasons.
A known technique to improve this algorithmic complexity is called \emph{spatial partitioning}. An explanation (and application to multiprocessor scaling) can, for instance, be found in \cite{plimpton1995fast}.

By splitting the world into smaller partitions, or \emph{boxes}, one only has to check for interactions between particles in nearby partitions.
In the ideal case, if the number of interacting particles contained per box is a constant $x$, the amount of boxes becomes $n/x$. Interactions need only be checked between particles in the same box for $x(x-1)/2$ pairs, and between particles in a given box and particles in the adjacent boxes. If each box has $z$ neighbouring boxes, then there are $zx^2/2$ such pairs. The total complexity of this problem thus reduces to 
$O\left(
	\left[ \frac{x(x-1)}{2} + \frac{zx^2}{2} \right]
		\cdot \frac{n}{x}
\right) = O(n)$,
linear in the number of interacting particles.

It is easy to show that we cannot do any better than $O(n)$ via this partitioning trick, and that choosing the number of boxes $b$ to be proportional to $n$ is the ideal case.
Indeed, assume that we would instead take the number of boxes $b \sim n^\alpha$. Then the average number of particles per box $x$ would be proportional to $n^{1 - \alpha}$. Using the reasoning above, we find that the average number of interaction pairs that would be considered is proportional to $x^2 b = n^{2(1 - \alpha)} n^\alpha = n^{2 -\alpha}$. It seems that we can make the algorithmic complexity sub-linear by choosing $\alpha > 1$, however, we also have to iterate over all the boxes, and their number is proportional to $n^\alpha$. In other words, the total algorithmic time complexity is $O(n^{2 - \alpha} + n^\alpha)$, with the optimal value of $\alpha$ equal to one.


\input{images/performancePerNumBox}
\input{images/numBoxExponentFit}
\input{images/spacePartVsNoSpacePart}

The performance of our implementation for various strand lengths and number of boxes (per dimension) is shown in figure \ref{performancePerNumBox}. All performance simulations were run on an Intel Core2 Duo E8500 CPU. Note that there is an upper bound on the maximum number of boxes per dimension. Indeed, the length of a box cannot fall short of the potential truncation distance (in our case: $20$\,\Angstrom).

The ideal number of boxes for each strand length in figure \ref{performancePerNumBox} is plotted in figure \ref{numBoxExponentFit}. We find the relation $\bperdim \sim n^{0.43 \pm 0.07}$, where $\bperdim$ is the number of boxes in each dimension of the world (i.e, the total number of boxes $b$ is $(\bperdim)^3$).
This exponent $\bperdim$ is close to the value 1/3, implied by the theoretical argument above. The slightly higher value is possibly due to the fact that a world filled with a DNA strand is far less homogeneous than a world filled with a gas, or randomly bouncing particles.
Hence, there will be a significant amount of boxes that are completely empty, and vice versa a significant amount of boxes with a much higher than average number of particles (sites).
Caching in modern CPUs may also play a role, in the sense that large numbers of monomers and boxes imply that the entire working set does not fit in the fastest cache levels on the CPU, potentially giving a bias to smaller boxes (though we can't immediately see a reason for this).

Lastly, the performance scaling with the ideal number of boxes is compared to the scaling without space partitioning in Figure \ref{spacePartVsNoSpacePart}. The quadratic trend of the naive method is clearly worse than the linear scaling when space partitioning is enabled.

